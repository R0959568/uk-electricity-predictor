{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "194e4d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading a small fallback sample from demanddata_2001.csv\n",
      "‚úÖ Cleaning function created and tested!\n",
      "Original columns: ['SETTLEMENT_DATE', 'SETTLEMENT_PERIOD', 'ND', 'TSD', 'ENGLAND_WALES_DEMAND', 'EMBEDDED_WIND_GENERATION', 'EMBEDDED_WIND_CAPACITY', 'EMBEDDED_SOLAR_GENERATION', 'EMBEDDED_SOLAR_CAPACITY', 'NON_BM_STOR', 'PUMP_STORAGE_PUMPING', 'SCOTTISH_TRANSFER', 'IFA_FLOW', 'IFA2_FLOW', 'BRITNED_FLOW', 'MOYLE_FLOW', 'EAST_WEST_FLOW', 'NEMO_FLOW', 'NSL_FLOW', 'ELECLINK_FLOW']\n",
      "New columns: ['settlement_date', 'settlement_period', 'nd', 'tsd', 'england_wales_demand', 'embedded_wind_generation', 'embedded_wind_capacity', 'embedded_solar_generation', 'embedded_solar_capacity', 'non_bm_stor', 'pump_storage_pumping', 'scottish_transfer', 'ifa_flow', 'ifa2_flow', 'britned_flow', 'moyle_flow', 'east_west_flow', 'nemo_flow', 'nsl_flow', 'eleclink_flow', 'viking_flow', 'greenlink_flow', 'year', 'month', 'day', 'hour', 'minute', 'date', 'demand_value', 'source_file']\n",
      "New shape: (500, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_period</th>\n",
       "      <th>nd</th>\n",
       "      <th>tsd</th>\n",
       "      <th>england_wales_demand</th>\n",
       "      <th>embedded_wind_generation</th>\n",
       "      <th>embedded_wind_capacity</th>\n",
       "      <th>embedded_solar_generation</th>\n",
       "      <th>embedded_solar_capacity</th>\n",
       "      <th>non_bm_stor</th>\n",
       "      <th>...</th>\n",
       "      <th>viking_flow</th>\n",
       "      <th>greenlink_flow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>date</th>\n",
       "      <th>demand_value</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38631.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34060.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>34060.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39808.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35370.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>35370.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40039.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35680.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>35680.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39339.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35029.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>35029.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38295.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>34047.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  settlement_date  settlement_period       nd  tsd  england_wales_demand  \\\n",
       "0      2001-01-01                1.0  38631.0  NaN               34060.0   \n",
       "1      2001-01-01                2.0  39808.0  NaN               35370.0   \n",
       "2      2001-01-01                3.0  40039.0  NaN               35680.0   \n",
       "3      2001-01-01                4.0  39339.0  NaN               35029.0   \n",
       "4      2001-01-01                5.0  38295.0  NaN               34047.0   \n",
       "\n",
       "   embedded_wind_generation  embedded_wind_capacity  \\\n",
       "0                       NaN                     NaN   \n",
       "1                       NaN                     NaN   \n",
       "2                       NaN                     NaN   \n",
       "3                       NaN                     NaN   \n",
       "4                       NaN                     NaN   \n",
       "\n",
       "   embedded_solar_generation  embedded_solar_capacity  non_bm_stor  ...  \\\n",
       "0                        NaN                      NaN          0.0  ...   \n",
       "1                        NaN                      NaN          0.0  ...   \n",
       "2                        NaN                      NaN          0.0  ...   \n",
       "3                        NaN                      NaN          0.0  ...   \n",
       "4                        NaN                      NaN          0.0  ...   \n",
       "\n",
       "   viking_flow  greenlink_flow  year  month  day  hour  minute        date  \\\n",
       "0          NaN             NaN  2001      1    1     0       0  2001-01-01   \n",
       "1          NaN             NaN  2001      1    1     0       0  2001-01-01   \n",
       "2          NaN             NaN  2001      1    1     0       0  2001-01-01   \n",
       "3          NaN             NaN  2001      1    1     0       0  2001-01-01   \n",
       "4          NaN             NaN  2001      1    1     0       0  2001-01-01   \n",
       "\n",
       "   demand_value  source_file  \n",
       "0       34060.0         <NA>  \n",
       "1       35370.0         <NA>  \n",
       "2       35680.0         <NA>  \n",
       "3       35029.0         <NA>  \n",
       "4       34047.0         <NA>  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.0 ‚Äî Clean column names and data types for electricity demand (chunk)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "def clean_electricity_data_chunk(chunk):\n",
    "    \"\"\"Clean a chunk/dataframe of electricity demand records.\n",
    "\n",
    "    Actions:\n",
    "    - normalize column names (lowercase, underscores)\n",
    "    - parse a timestamp-like column to datetime if present\n",
    "    - extract date/time features (year, month, day, hour, minute)\n",
    "    - coerce demand/load columns to numeric (float64)\n",
    "    - strip text columns and preserve provenance\n",
    "\n",
    "    The function enforces consistent dtypes for numeric columns (float64) and uses\n",
    "    pandas nullable `Int64` for integer-like date fields to reduce parquet schema\n",
    "    mismatches across chunks.\n",
    "    \"\"\"\n",
    "    df = chunk.copy()\n",
    "\n",
    "    # 1) Normalize column names\n",
    "    def _clean_colname(c):\n",
    "        c = str(c).strip()\n",
    "        c = c.replace(\" \", \"_\")\n",
    "        c = c.replace(\"/\", \"_\")\n",
    "        c = c.replace(\"-\", \"_\")\n",
    "        c = c.replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "        # lowercase and collapse repeated underscores\n",
    "        c = \"_\".join([p for p in c.lower().split(\"_\") if p])\n",
    "        return c\n",
    "\n",
    "    df.columns = [_clean_colname(c) for c in df.columns]\n",
    "\n",
    "    # 2) Identify a timestamp column (typical names: date, time, timestamp, datetime, settlement_date)\n",
    "    ts_candidates = [c for c in df.columns if any(k in c for k in ['date', 'time', 'timestamp', 'datetime', 'settlement'])]\n",
    "    ts_col = ts_candidates[0] if ts_candidates else None\n",
    "\n",
    "    if ts_col is not None:\n",
    "        # parse datetimes defensively\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
    "        # use pandas nullable integer dtype for date parts\n",
    "        df['year'] = df[ts_col].dt.year.astype('Int64')\n",
    "        df['month'] = df[ts_col].dt.month.astype('Int64')\n",
    "        df['day'] = df[ts_col].dt.day.astype('Int64')\n",
    "        df['hour'] = df[ts_col].dt.hour.astype('Int64')\n",
    "        df['minute'] = df[ts_col].dt.minute.astype('Int64')\n",
    "        df['date'] = df[ts_col].dt.date\n",
    "    else:\n",
    "        df['year'] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "        df['month'] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "        df['day'] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "        df['hour'] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "        df['minute'] = pd.Series([pd.NA] * len(df), dtype='Int64')\n",
    "        df['date'] = pd.Series([pd.NA] * len(df))\n",
    "\n",
    "    # 3) Coerce obvious numeric demand/load columns to numeric (float64)\n",
    "    demand_candidates = [c for c in df.columns if any(k in c for k in ['demand', 'load', 'mw', 'mwh', 'kw', 'kwh', 'total', 'value'])]\n",
    "\n",
    "    for c in demand_candidates:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce').astype('float64')\n",
    "\n",
    "    # Also coerce other numeric-like columns to float64 for consistent schema\n",
    "    for c in df.columns:\n",
    "        if c not in demand_candidates and c not in ['year', 'month', 'day', 'hour', 'minute']:\n",
    "            # try to coerce columns that look numeric\n",
    "            if df[c].dtype == object:\n",
    "                # heuristic: if >50% of non-null values are numeric, coerce\n",
    "                nonnull = df[c].dropna()\n",
    "                if len(nonnull) > 0:\n",
    "                    # cast to string first to safely use string operations\n",
    "                    nonnull_str = nonnull.astype(str)\n",
    "                    num_like = nonnull_str.str.replace(r\"[^0-9eE+-.]\", \"\", regex=True).str.match(r\"^[+-]?[0-9]*\\.?[0-9]+(?:[eE][+-]?[0-9]+)?$\")\n",
    "                    if num_like.mean() > 0.5:\n",
    "                        df[c] = pd.to_numeric(df[c], errors='coerce').astype('float64')\n",
    "\n",
    "    # 4) Normalise text columns: strip\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 5) Keep a stable primary demand column if found, and create `demand_value` alias\n",
    "    primary_demand = None\n",
    "    for c in demand_candidates:\n",
    "        if 'demand' in c:\n",
    "            primary_demand = c\n",
    "            break\n",
    "    if primary_demand is None and demand_candidates:\n",
    "        primary_demand = demand_candidates[0]\n",
    "\n",
    "    if primary_demand is not None:\n",
    "        df['demand_value'] = df[primary_demand]\n",
    "    else:\n",
    "        df['demand_value'] = pd.Series([pd.NA] * len(df), dtype='float64')\n",
    "\n",
    "    # 6) Preserve provenance\n",
    "    if 'source_file' not in df.columns:\n",
    "        df['source_file'] = pd.Series([pd.NA] * len(df))\n",
    "    else:\n",
    "        df['source_file'] = df['source_file'].astype(str)\n",
    "\n",
    "    # Ensure consistent dtypes: cast numeric columns to float64 and date parts to Int64\n",
    "    for c in df.select_dtypes(include=['number']).columns:\n",
    "        df[c] = df[c].astype('float64')\n",
    "\n",
    "    for dcol in ['year', 'month', 'day', 'hour', 'minute']:\n",
    "        if dcol in df.columns:\n",
    "            df[dcol] = df[dcol].astype('Int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test the cleaner on available data: prefer `df_all` (full read), then `df_sample`, then read first CSV\n",
    "if 'df_all' in globals():\n",
    "    df_test = df_all.head(1000)\n",
    "    print(\"Testing `clean_electricity_data_chunk` on `df_all.head(1000)`\")\n",
    "elif 'df_sample' in globals():\n",
    "    df_test = df_sample\n",
    "    print(\"Testing `clean_electricity_data_chunk` on `df_sample`\")\n",
    "else:\n",
    "    # try reading the first CSV file as a fallback\n",
    "    data_dir = Path(\"../Dataset_2_UK_Historic_Electricity_Demand_Data\")\n",
    "    csv_files = sorted(data_dir.glob(\"*.csv\"))\n",
    "    if csv_files:\n",
    "        print(f\"Reading a small fallback sample from {csv_files[0].name}\")\n",
    "        df_test = pd.read_csv(csv_files[0], nrows=500)\n",
    "    else:\n",
    "        df_test = None\n",
    "        print(\"No data available to test the cleaning function.\")\n",
    "\n",
    "if df_test is not None:\n",
    "    df_cleaned_sample = clean_electricity_data_chunk(df_test)\n",
    "    print(\"‚úÖ Cleaning function created and tested!\")\n",
    "    print(f\"Original columns: {list(df_test.columns)[:20]}\")\n",
    "    print(f\"New columns: {list(df_cleaned_sample.columns)[:30]}\")\n",
    "    print(f\"New shape: {df_cleaned_sample.shape}\")\n",
    "    display(df_cleaned_sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdcf6470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_strategic_subset_elec ready: streams all CSVs in folder, writes cleaned full dataset (parquet preferred), and creates unbiased sample (reservoir sampling).\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Create strategic subset for electricity (streaming -> parquet) with reservoir sampling\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "def create_strategic_subset_elec(csv_dir,\n",
    "                                 start_year=2001,\n",
    "                                 end_year=2025,\n",
    "                                 out_dir='data/interim',\n",
    "                                 sample_n=500,\n",
    "                                 chunksize=200_000,\n",
    "                                 parquet_engine_preference=('pyarrow', 'fastparquet')):\n",
    "    \"\"\"Stream all CSVs in `csv_dir`, clean chunks using `clean_electricity_data_chunk`,\n",
    "    write a cleaned full dataset.\n",
    "\n",
    "    Uses reservoir sampling during streaming to produce unbiased sample (memory-bounded).\n",
    "    Now with schema standardization to handle column evolution across years.\n",
    "    \"\"\"\n",
    "    csv_dir = Path(csv_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_stem = 'elec_cleaned_full'\n",
    "    full_parquet = out_dir / f\"{full_stem}.parquet\"\n",
    "    full_csv_out = out_dir / f\"{full_stem}.csv\"\n",
    "    full_sample_path = out_dir / f\"{full_stem}_sample.csv\"\n",
    "\n",
    "    # Parquet engine detection\n",
    "    parquet_engine = None\n",
    "    for eng in parquet_engine_preference:\n",
    "        try:\n",
    "            import importlib\n",
    "            importlib.import_module(eng)\n",
    "            parquet_engine = eng\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    use_parquet = parquet_engine is not None\n",
    "\n",
    "    # Setup pyarrow writers if available\n",
    "    parquet_writer_full = None\n",
    "    pa = None\n",
    "    pq = None\n",
    "    if use_parquet and parquet_engine == 'pyarrow':\n",
    "        try:\n",
    "            import pyarrow as pa\n",
    "            import pyarrow.parquet as pq\n",
    "        except Exception:\n",
    "            use_parquet = False\n",
    "\n",
    "    total_written = 0\n",
    "    first_write_full = True\n",
    "    first_write_csv = True\n",
    "\n",
    "    # Pre-check existence\n",
    "    parquet_exists = full_parquet.exists() if use_parquet else False\n",
    "    csv_exists = full_csv_out.exists()\n",
    "\n",
    "    if parquet_exists and csv_exists:\n",
    "        print(f\"Full cleaned outputs already exist; will skip writing:\")\n",
    "        print(f\"  - Parquet: {full_parquet}\")\n",
    "        print(f\"  - CSV: {full_csv_out}\")\n",
    "        full_exists = True\n",
    "    else:\n",
    "        full_exists = False\n",
    "        if parquet_exists:\n",
    "            print(f\"Parquet exists but CSV missing. Will create CSV: {full_csv_out}\")\n",
    "        if csv_exists:\n",
    "            print(f\"CSV exists but Parquet missing. Will create Parquet: {full_parquet}\")\n",
    "\n",
    "    # Reservoir for streaming unbiased sample\n",
    "    full_reservoir = []\n",
    "    full_count = 0\n",
    "\n",
    "    # ===== SCHEMA DETECTION: Scan all files to get complete column set =====\n",
    "    master_schema = None\n",
    "    all_columns = set()\n",
    "    \n",
    "    csv_files = sorted(csv_dir.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {csv_dir}\")\n",
    "        return {}\n",
    "\n",
    "    if not full_exists:\n",
    "        print(\"üîç Phase 1: Detecting complete schema across all years...\")\n",
    "        for fpath in csv_files:\n",
    "            try:\n",
    "                # Read just first chunk to get column names\n",
    "                sample_chunk = pd.read_csv(fpath, nrows=100, low_memory=False)\n",
    "                sample_cleaned = clean_electricity_data_chunk(sample_chunk)\n",
    "                all_columns.update(sample_cleaned.columns)\n",
    "            except Exception as exc:\n",
    "                print(f\"Warning: could not scan {fpath.name}: {exc}\")\n",
    "        \n",
    "        # Sort columns for consistent ordering\n",
    "        all_columns = sorted(list(all_columns))\n",
    "        print(f\"‚úÖ Detected {len(all_columns)} unique columns across all years\")\n",
    "        \n",
    "        print(\"\\n‚öôÔ∏è Phase 2: Processing and writing data with standardized schema...\")\n",
    "        \n",
    "        # Iterate all CSV files and stream their chunks\n",
    "        for f_i, fpath in enumerate(csv_files):\n",
    "            try:\n",
    "                for i, chunk in enumerate(pd.read_csv(fpath, chunksize=chunksize, low_memory=False)):\n",
    "                    chunk_cleaned = clean_electricity_data_chunk(chunk)\n",
    "\n",
    "                    # ===== SCHEMA STANDARDIZATION: Add missing columns with proper dtype =====\n",
    "                    import numpy as np\n",
    "                    for col in all_columns:\n",
    "                        if col not in chunk_cleaned.columns:\n",
    "                            # Infer dtype from existing columns: use appropriate NaN for each type\n",
    "                            if col in ['year', 'month', 'day', 'hour', 'minute']:\n",
    "                                chunk_cleaned[col] = pd.Series([pd.NA] * len(chunk_cleaned), dtype='Int64')\n",
    "                            elif col in ['date', 'settlement_date', 'source_file']:\n",
    "                                chunk_cleaned[col] = None  # Will be treated as null strings\n",
    "                            else:\n",
    "                                # Use numpy nan for float columns (PyArrow compatible)\n",
    "                                chunk_cleaned[col] = np.nan\n",
    "                    \n",
    "                    # Reorder columns to match master schema\n",
    "                    chunk_cleaned = chunk_cleaned[all_columns]\n",
    "\n",
    "                    # ===== write to full cleaned output (parquet and CSV) =====\n",
    "                    try:\n",
    "                        # Write Parquet\n",
    "                        if use_parquet and parquet_engine == 'pyarrow' and not parquet_exists:\n",
    "                            table_full = pa.Table.from_pandas(chunk_cleaned)\n",
    "                            if parquet_writer_full is None:\n",
    "                                # Establish schema from first standardized chunk\n",
    "                                master_schema = table_full.schema\n",
    "                                parquet_writer_full = pq.ParquetWriter(str(full_parquet), master_schema)\n",
    "                            parquet_writer_full.write_table(table_full)\n",
    "                        \n",
    "                        # Write CSV (complete dataset) with proper type conversion\n",
    "                        if not csv_exists:\n",
    "                            # Prepare CSV-friendly version (convert to dict then back to DataFrame)\n",
    "                            # This approach matches the sample CSV and properly handles Int64 -> int64\n",
    "                            chunk_csv = pd.DataFrame(chunk_cleaned.to_dict('records'))\n",
    "                            \n",
    "                            # Convert date objects to strings if present\n",
    "                            if 'date' in chunk_csv.columns and chunk_csv['date'].dtype == object:\n",
    "                                chunk_csv['date'] = chunk_csv['date'].astype(str)\n",
    "                            if 'settlement_date' in chunk_csv.columns and chunk_csv['settlement_date'].dtype == object:\n",
    "                                chunk_csv['settlement_date'] = chunk_csv['settlement_date'].astype(str)\n",
    "                            \n",
    "                            if first_write_csv:\n",
    "                                chunk_csv.to_csv(full_csv_out, index=False, mode='w', na_rep='NaN')\n",
    "                                first_write_csv = False\n",
    "                            else:\n",
    "                                chunk_csv.to_csv(full_csv_out, index=False, header=False, mode='a', na_rep='NaN')\n",
    "                        \n",
    "                        first_write_full = False\n",
    "                        total_written += len(chunk_cleaned)\n",
    "                    except Exception as e:\n",
    "                        print(f'Warning: failed to write chunk from {fpath.name}:', e)\n",
    "\n",
    "                    # ===== update reservoir for unbiased sampling =====\n",
    "                    for _, row in chunk_cleaned.iterrows():\n",
    "                        full_count += 1\n",
    "                        if len(full_reservoir) < sample_n:\n",
    "                            full_reservoir.append(row.to_dict())\n",
    "                        else:\n",
    "                            s = random.randint(1, full_count)\n",
    "                            if s <= sample_n:\n",
    "                                idx = random.randint(0, sample_n - 1)\n",
    "                                full_reservoir[idx] = row.to_dict()\n",
    "\n",
    "                    # periodic progress\n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        print(f\"Processed chunk {i+1} of file {fpath.name}. Total rows so far: {total_written}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"Warning: failed to stream {fpath.name}: {exc}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Phase 2 complete: Processed all {len(csv_files)} files\")\n",
    "\n",
    "    # Close writer\n",
    "    try:\n",
    "        if use_parquet and parquet_engine == 'pyarrow':\n",
    "            if parquet_writer_full is not None:\n",
    "                parquet_writer_full.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Persist reservoir sample to CSV\n",
    "    try:\n",
    "        if len(full_reservoir) > 0:\n",
    "            pd.DataFrame(full_reservoir).head(sample_n).to_csv(full_sample_path, index=False, na_rep='NaN')\n",
    "        else:\n",
    "            pd.DataFrame().to_csv(full_sample_path, index=False, na_rep='NaN')\n",
    "    except Exception as e:\n",
    "        print('Warning: failed to persist reservoir sample due to:', e)\n",
    "        pd.DataFrame().to_csv(full_sample_path, index=False, na_rep='NaN')\n",
    "\n",
    "    result = {\n",
    "        'full_parquet': str(full_parquet) if use_parquet else None,\n",
    "        'full_csv': str(full_csv_out),\n",
    "        'full_sample': str(full_sample_path),\n",
    "        'rows_written': int(total_written)\n",
    "    }\n",
    "\n",
    "    print(f\"Done. Total rows written: {total_written}\")\n",
    "    if result['full_parquet']:\n",
    "        print(f\"Full cleaned Parquet file: {result['full_parquet']}\")\n",
    "    print(f\"Full cleaned CSV file: {result['full_csv']}\")\n",
    "    print(f\"Sample file: {result['full_sample']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print('create_strategic_subset_elec ready: streams all CSVs in folder, writes cleaned full dataset (parquet preferred), and creates unbiased sample (reservoir sampling).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "592a2fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting create_strategic_subset_elec for full dataset (this will stream and write files)...\n",
      "Parquet exists but CSV missing. Will create CSV: data/interim/elec_cleaned_full.csv\n",
      "üîç Phase 1: Detecting complete schema across all years...\n",
      "‚úÖ Detected 30 unique columns across all years\n",
      "\n",
      "‚öôÔ∏è Phase 2: Processing and writing data with standardized schema...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
      "/var/folders/f0/_tj0pt9x4031t82dhd03xkz00000gn/T/ipykernel_93765/1567972675.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Phase 2 complete: Processed all 25 files\n",
      "Done. Total rows written: 434014\n",
      "Full cleaned Parquet file: data/interim/elec_cleaned_full.parquet\n",
      "Full cleaned CSV file: data/interim/elec_cleaned_full.csv\n",
      "Sample file: data/interim/elec_cleaned_full_sample.csv\n",
      "\n",
      "result: {'full_parquet': 'data/interim/elec_cleaned_full.parquet', 'full_csv': 'data/interim/elec_cleaned_full.csv', 'full_sample': 'data/interim/elec_cleaned_full_sample.csv', 'rows_written': 434014}\n",
      "\n",
      "Outputs existence:\n",
      "- data/interim/elec_cleaned_full.parquet: True\n",
      "- data/interim/elec_cleaned_full_sample.csv: True\n",
      "\n",
      "Contents of data/interim: ['elec_cleaned_full.csv', 'elec_cleaned_full.parquet', 'elec_cleaned_full_sample.csv']\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Runner: execute the streaming ETL for electricity and verify outputs\n",
    "# Run this cell after `clean_electricity_data_chunk` and `create_strategic_subset_elec`\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    print(\"Starting create_strategic_subset_elec for full dataset (this will stream and write files)...\")\n",
    "    res = create_strategic_subset_elec(\"../Dataset_2_UK_Historic_Electricity_Demand_Data\")\n",
    "    print('\\nresult:', res)\n",
    "\n",
    "    out_paths = []\n",
    "    if res.get('full_parquet'):\n",
    "        out_paths.append(Path(res['full_parquet']))\n",
    "    elif res.get('full_csv'):\n",
    "        out_paths.append(Path(res['full_csv']))\n",
    "\n",
    "    if res.get('full_sample'):\n",
    "        out_paths.append(Path(res['full_sample']))\n",
    "\n",
    "    exist_map = {str(p): p.exists() for p in out_paths}\n",
    "    print('\\nOutputs existence:')\n",
    "    for p, exists in exist_map.items():\n",
    "        print(f\"- {p}: {exists}\")\n",
    "\n",
    "    interim = Path('data/interim')\n",
    "    if interim.exists():\n",
    "        listing = sorted([p.name for p in interim.iterdir()])\n",
    "    else:\n",
    "        listing = []\n",
    "    print('\\nContents of data/interim:', listing)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Runner failed with exception:', e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2220e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Regenerate full CSV from Parquet with consistent null handling\n",
    "# Run this cell to ensure elec_cleaned_full.csv has proper empty cell formatting\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def reformat_full_csv_with_standard_nulls(parquet_path, csv_path):\n",
    "    \"\"\"\n",
    "    Regenerate the full CSV from Parquet with industry-standard null handling.\n",
    "    Writes explicit 'NaN' text for null values (industry standard for CSV).\n",
    "    \"\"\"\n",
    "    print(f\"Reading Parquet file: {parquet_path}\")\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    \n",
    "    print(f\"Converting to dict and back to DataFrame for consistent formatting...\")\n",
    "    # This approach converts Int64 ‚Üí int64, proper handling of types\n",
    "    df_csv = pd.DataFrame(df.to_dict('records'))\n",
    "    \n",
    "    # Convert date columns to strings if present\n",
    "    for col in ['date', 'settlement_date']:\n",
    "        if col in df_csv.columns and df_csv[col].dtype == object:\n",
    "            df_csv[col] = df_csv[col].astype(str)\n",
    "    \n",
    "    print(f\"Writing CSV with industry-standard null handling (explicit NaN): {csv_path}\")\n",
    "    # na_rep='NaN' writes explicit NaN text for null values (industry standard)\n",
    "    df_csv.to_csv(csv_path, index=False, na_rep='NaN')\n",
    "    \n",
    "    print(f\"‚úÖ Done! CSV file regenerated with standard null handling.\")\n",
    "    print(f\"   Rows: {len(df_csv)}, Columns: {len(df_csv.columns)}\")\n",
    "    \n",
    "    # Show null handling\n",
    "    null_counts = df_csv.isnull().sum()\n",
    "    print(f\"\\n   Null values per column (top 5):\")\n",
    "    print(null_counts[null_counts > 0].head())\n",
    "    \n",
    "    return df_csv\n",
    "\n",
    "# Execute the reformatting\n",
    "parquet_file = Path('data/interim/elec_cleaned_full.parquet')\n",
    "csv_file = Path('data/interim/elec_cleaned_full.csv')\n",
    "\n",
    "if parquet_file.exists():\n",
    "    df_result = reformat_full_csv_with_standard_nulls(parquet_file, csv_file)\n",
    "    \n",
    "    # Verify the result\n",
    "    print(\"\\nüìä Verification:\")\n",
    "    print(f\"Data types:\\n{df_result.dtypes.value_counts()}\")\n",
    "    print(f\"\\nFirst 3 rows with nulls visible:\")\n",
    "    print(df_result.head(3).to_string())\n",
    "else:\n",
    "    print(f\"‚ùå Parquet file not found: {parquet_file}\")\n",
    "    print(\"Run cell 3 first to generate the Parquet file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
